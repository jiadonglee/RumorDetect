{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wboKmlUIIoPG",
        "colab_type": "text"
      },
      "source": [
        "# Mount Google Drive to the Dataset\n",
        "## If run locally, do not need to run this section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FLf_Psgqzjh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "68ed03e5-b62e-4d97-f51d-ff31daf01b73"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNvr_PadrHAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c7d84ff-0801-4e7c-a677-1ed95244f311"
      },
      "source": [
        "cd /content/gdrive/My Drive/Project/Media"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Project/Media\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtQXVA0LrOQy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "be3f880c-ba6d-4181-fd05-a20f6170678a"
      },
      "source": [
        "ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "content_seg.txt  med250.model.bin  Sntlst.txt   wordMatrix.npz\n",
            "\u001b[0m\u001b[01;34mDataset\u001b[0m/         Preprocess.ipynb  train.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqUtYp0LClcK",
        "colab_type": "text"
      },
      "source": [
        "# LSTM Text Classsifier by Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGH-m7MnnpAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn import decomposition, ensemble\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dense, Input, Flatten, Dropout\n",
        "from keras.layers import LSTM, Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9N91tqMDCfU",
        "colab_type": "text"
      },
      "source": [
        "###  Hyper Parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4wXjaGGn_YO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 60 # max length of each weibo\n",
        "EMBEDDING_DIM = 100 # dimensions of word embedding\n",
        "TEST_SPLIT = 0.2 # split the data to train and test data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwQcKbLOnpAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Word2Vec Model & Word Embedding Matrix\n",
        "\n",
        "w2v_model = Word2Vec.load(\"Model/med250.model.bin\")\n",
        "word_file = np.load(\"Model/wordMatrix.npz\")\n",
        "embedding_matrix = word_file['arr_0']\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Get texts of aimed file\n",
        "def getTexts():\n",
        "    Texts = []\n",
        "    with open(\"Sntlst.txt\", \"r\", encoding='utf8') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            Texts.append(line.strip())\n",
        "    return Texts\n",
        "all_texts = getTexts()\n",
        "\n",
        "# Make texts to sequences\n",
        "tokenizer.fit_on_texts(all_texts)\n",
        "sequences = tokenizer.texts_to_sequences(all_texts)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print('Shape of data tensor:', data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnBz3FmKnpAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ddf55a11-f7d7-4ab1-c308-e2020225348a"
      },
      "source": [
        "# Split Train and Test Data\n",
        "y_0 = [[0] * 7907]\n",
        "y_1 = [[1] * 7880]\n",
        "y = np.append(y_0, y_1)\n",
        "labels = to_categorical(np.asarray(y))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=42)\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10577, 60) (5210, 60)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEL2dETanpAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "65897c7a-5388-4172-f574-6e29e64dc3ec"
      },
      "source": [
        "# Get Embedding Matrix of Pretrained Word2Vec Model\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items(): \n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = np.asarray(w2v_model.wv[word],\n",
        "                                         dtype='float32')\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(59128, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2WbxzzNnpAk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "e0c0a8ab-5aef-46ce-e2d8-3833bc3259e9"
      },
      "source": [
        "# Defined Embedding Layer\n",
        "embedding_layer = Embedding(input_dim = len(word_index) + 1,\n",
        "                            output_dim = EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)\n",
        "\n",
        "# LSTM Model\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "model.fit(X_train, y_train, epochs=2, batch_size=128)\n",
        "model.save('Model/word_vector.h5')\n",
        "print(model.evaluate(X_test, y_test))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 60, 100)           5912800   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 5,993,402\n",
            "Trainable params: 80,602\n",
            "Non-trainable params: 5,912,800\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "10577/10577 [==============================] - 21s 2ms/step - loss: 0.0277 - acc: 0.9887\n",
            "Epoch 2/2\n",
            "10577/10577 [==============================] - 19s 2ms/step - loss: 0.0099 - acc: 0.9975\n",
            "5210/5210 [==============================] - 3s 587us/step\n",
            "[0.004930232965489965, 0.9990403071017274]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYnMcPIUnpAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}